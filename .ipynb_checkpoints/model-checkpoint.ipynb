{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "base_dir = root_dir + '/ML dataSets/TV Scripting'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ML starts, above is the code for colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, LSTM ,Dropout,Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"./scriptsFor3Seasons.txt\"\n",
    "with open(data_dir) as f:\n",
    "    data=f.read()\n",
    "\n",
    "data=data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n                    \\t\\t\\t- so, what do you think? - wow.\\n it's for you, right? it's for both of us.\\n don't go away.\\n don't worry.\\n there's not enough blood left in my legs to go anywhere.\\n hey, it's charlie.\\n do your thing when you hear the beep.\\n listen, you lousy s.\\no.\\nb.\\n, i will not be treated like this.\\n either you call me, or you are gonna be very, very sorry.\\n i love you, monkey man.\\n charlie? who was that? damn telemarketers.\\n a telemarketer who calls you monkey man? i'm on some weir\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                    \\t\\t\\t- so, what do you think? - wow.\\n it's for you, right? it's for both of us.\\n d\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[6:] \n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                    \\t\\t\\t- so, what do you think? - wow.',\n",
       " \" it's for you, right? it's for both of us.\",\n",
       " \" don't go away.\",\n",
       " \" don't worry.\",\n",
       " \" there's not enough blood left in my legs to go anywhere.\",\n",
       " \" hey, it's charlie.\",\n",
       " ' do your thing when you hear the beep.',\n",
       " ' listen, you lousy s.',\n",
       " 'o.',\n",
       " 'b.',\n",
       " ', i will not be treated like this.',\n",
       " ' either you call me, or you are gonna be very, very sorry.',\n",
       " ' i love you, monkey man.',\n",
       " ' charlie']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:400].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting S.O.B as different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "punch=['.','[',']','(',')',';',':',\"'\",\"/\",'\"',',','?','*','!','-','$','%',\n",
    "       '&','\\n']\n",
    "\n",
    "for i in punch:\n",
    "    data=data.replace(i,' '+i+' ')\n",
    "data=data.replace('\\n','<newline>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                    \\t\\t\\t -  so ,  what do you think ?   -  wow . <newline> it ' s for you ,  right ?  it ' s for both of us . <newline> don ' t go away . <newline> don ' t worry . <newline> there ' s not enough blood left in my legs to go anywhere . <newline> hey ,  it ' s charlie . <newline> do your thing when you hear the beep . <newline> listen ,  you lousy s . <newline>o . <newline>b . <newline\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(text):\n",
    "    counter=Counter()\n",
    "    int_to_vocab=dict()\n",
    "    vocab_to_int=dict()\n",
    "    \n",
    "    for d in text.split():\n",
    "        counter[d]+=1\n",
    "    \n",
    "    index=0\n",
    "    for word in counter:\n",
    "        int_to_vocab[index]=word\n",
    "        vocab_to_int[word]=index\n",
    "        index+=1\n",
    "    \n",
    "    return counter, vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,vocab_to_int, int_to_vocab =get_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  9008\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size : \",len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverting the whole text corpus into integer corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_int=[]\n",
    "\n",
    "for word in data.split():\n",
    "    text_int.append(vocab_to_int[word])\n",
    "text_int=np.array(text_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are approximately number of words in Two and Half Men's 3 Sesaons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune these below hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "seq_len=100\n",
    "embedding=300\n",
    "lstm_size=128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that in the tutorial it is just one step ahead and ahead, I am making seq_len steps ahead, and see what that happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data,seq_len):\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    \n",
    "    for i in range(0,len(data)-seq_len,seq_len):\n",
    "        x=data[i:i+seq_len]\n",
    "        y=data[i+1:i+seq_len+1]\n",
    "        \n",
    "        x_train.append(np.array(x))\n",
    "        y_train.append(np.array(y))\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y =get_training_data(text_int,seq_len)\n",
    "\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "y=y.reshape(y.shape[0],y.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2593, 100), (2593, 100, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp=Input((None,))\n",
    "\n",
    "embed=Embedding(input_dim=vocab_size,output_dim=embedding)\n",
    "lstm1=LSTM(lstm_size,return_sequence=True,return_state=True)\n",
    "lstm2=LSTM(lstm_size,return_sequence=True,return_state=True)\n",
    "lstm3=LSTM(lstm_size,return_sequence=True,return_state=True)\n",
    "dense=Dense(vocab_size)\n",
    "\n",
    "net=embed(inp)\n",
    "net, h1, c1=lstm1(net)\n",
    "net, h2, c2=lstm2(net)\n",
    "net, h3, c3=lstm3(net)\n",
    "out=dense(net)\n",
    "\n",
    "model=Model(inp,out)\n",
    "\n",
    "init_state=[Input((lstm_size,)) for i in range(6)]\n",
    "\n",
    "inference=embed(inp)\n",
    "inference, h1, c1=lstm(inference,initial_state=init_states[:2])\n",
    "inference, h2, c2=lstm(inference,init_state=init_state[2:4])\n",
    "inference, h3, c3=lstm(inference,init_state=init_state[4:6])\n",
    "inf_out=dense(inference)\n",
    "\n",
    "states=[h1,c1,h2,c2,h3,c3]\n",
    "inf_model=Model([inp]+ init_states,[inf_out]+ states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimzer.lr=.01\n",
    "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy'\n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make validation set and change the hyper params as per best accuracy and see the things**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fit(x,y,batch_size=128,epochs=1,shuffle=True))\n",
    "model.save(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is finalised then extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(length,start):\n",
    "    \n",
    "    states=[np.zeros((1,lstm_size)) for i in range(6)]\n",
    "    \n",
    "    token=np.zeros((1,1))\n",
    "    token[0,0]=start\n",
    "    text=int_to_vocab[start] + ' '\n",
    "    \n",
    "    for i in range(length):\n",
    "        \n",
    "        out=inf_model.predict([token]+states)\n",
    "        word=np.argmax(out[0][0,0,:])\n",
    "        text+=int_to_vocab[word]\n",
    "        states=out[1:7]\n",
    "        token[0][0]=word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text=extract_text(100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_text(text):\n",
    "    \n",
    "    punch1=['.', ':', '!', ';', ')', ']', '?', ',', '%']\n",
    "    for i in punch1:\n",
    "        text = text.replace(' '+i, i)\n",
    "    punch2 = ['[', '(', '$']    \n",
    "    for i in punch2:\n",
    "        text = text.replace(i+' ', i)\n",
    "    punch3 = [\"'\", '-']    \n",
    "    for i in punch3:\n",
    "        text = text.replace(' '+i+' ', i)\n",
    "        \n",
    "    text = text.split('<newline>')  \n",
    "    for line in text:\n",
    "        if len(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_process_text(generated_text))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Anna_KaRNNa_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
